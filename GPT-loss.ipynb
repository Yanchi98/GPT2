{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手撕GPT2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造数据\n",
    "比如X(1,6,512), Y(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:torch.Size([1, 6, 512])\n",
      "Y.shape:torch.Size([1, 6])\n",
      "Y[:10]:tensor([[ 8230, 28369,  4677, 26946, 16820, 15115]])\n"
     ]
    }
   ],
   "source": [
    "B=1 # batch\n",
    "T=6 # length\n",
    "D=512 # dimension\n",
    "vocab_len = 32000\n",
    "\n",
    "X=torch.randn(B, T, D)\n",
    "Y=torch.randint(low=0, high=vocab_len, size=(B, T), dtype=torch.long)\n",
    "print(f'X.shape:{X.shape}')\n",
    "print(f'Y.shape:{Y.shape}')\n",
    "print(f'Y[:10]:{Y[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q = torch.randn(D,D)\n",
    "w_k = torch.randn(D,D)\n",
    "w_v = torch.randn(D,D)\n",
    "w_o = torch.randn(D,D)\n",
    "\n",
    "# mask是作用在序列T上的\n",
    "mask = torch.tril(torch.ones(T,T))\n",
    "print(mask)\n",
    "\n",
    "# 同w_q = nn.linear(D, D), w_q(X)\n",
    "Q,K,V = X @ w_q, X @ w_k, X @ w_v\n",
    "# 除根号d，因为dk增大，点积累计会很大，softmax计算的分布会很不均匀，接近于0-1分布，导致softmax数值不稳定\n",
    "# 带来梯度消失问题，增加根号dk，好处是softmax输出更平滑了，在不同维度dk下都能保证较好的训练效果\n",
    "scores = Q @ K.transpose(1,2) / math.sqrt(D)\n",
    "scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "weight = F.softmax(scores, dim=2)\n",
    "attn = weight @ V\n",
    "attn = attn @ w_o\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_up = torch.randn(D, 1024)\n",
    "mlp_down = torch.randn(1024, D)\n",
    "\n",
    "mlp = attn @ mlp_up @ mlp_down\n",
    "mlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 32000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取next token在词表上的概率\n",
    "lm_head = torch.randn(D, vocab_len)\n",
    "logits = mlp @ lm_head\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs.shape:torch.Size([1, 6, 32000])\n",
      "Y:tensor([[ 8230, 28369,  4677, 26946, 16820, 15115]])\n",
      "loss:10.373614311218262\n",
      "预测的next token:tensor([[21070, 24383, 26998, 24149, 26998, 22766]])\n",
      "torch.Size([1, 6])\n",
      "tensor([22766])\n"
     ]
    }
   ],
   "source": [
    "# 将logits的值转换到0-1之间\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# 6个token 对应 6个logits[32000]\n",
    "print(f'probs.shape:{probs.shape}')\n",
    "print(f'Y:{Y}')\n",
    "\n",
    "# Loss 函数，交叉熵\n",
    "# 以下两种计算loss的结构都正确，cross_entropy输入input(N,C), output(N), N为样本量，C为分类数量\n",
    "loss = F.cross_entropy(probs.view(-1, probs.size(-1)), Y.view(-1))\n",
    "loss = F.cross_entropy(probs.transpose(1, 2), Y)\n",
    "print(f'loss:{loss}')\n",
    "\n",
    "# 找到概率最大的token\n",
    "pred = torch.argmax(probs, dim=-1)\n",
    "# 训练时，每个tokne都对预测next token\n",
    "print(f\"预测的next token:{pred}\")\n",
    "print(pred.shape)\n",
    "print(pred[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
