{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 什么是 BPE-Tokenizer\n",
    "BPE-Tokenizer 使用 BPE 算法来将单词分解成更小的子词单元。BPE 算法最早用于数据压缩，但在 NLP 中，它通过学习字符级合并规则来实现分词。具体过程如下：\n",
    "\n",
    "STEP 1 初始状态：将训练语料库中的每个单词拆分成字符序列。\n",
    "例如，单词 \"hello\" 会拆分成字符序列 h e l l o。\n",
    "\n",
    "STEP 2 频次统计：统计字符对的出现频率（例如 \"h e\"、\"e l\" 等）。\n",
    "\n",
    "STEP 3 字符对合并：每次合并出现频率最高的字符对，并将其当作一个新的子词单元。例如，如果 \"e l\" 出现频率最高，则将其合并为 \"el\"，形成 h el l o。\n",
    "\n",
    "STEP 4 迭代：重复上述步骤，直到达到预设的合并次数或词表大小。\n",
    "\n",
    "最终，BPE 生成的词表包含了最常见的字符对和子词单元，这些单元可以组合成单词或短语，实现对文本的灵活分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BPE-Tokenizer 的特点\n",
    "### a. 处理未登录词（Out-of-Vocabulary, OOV）问题\n",
    "BPE-Tokenizer 的一个显著优点是能够有效地应对未登录词的问题。即使在词汇表中没有找到某个单词，BPE 也可以将其分解成多个已知的子词。例如，对于一个新词 \"unhappiness\"，即使词表中没有该词，BPE 也能将其分解为 \"un\"、\"happiness\"，甚至进一步拆分为更小的子词单元。\n",
    "\n",
    "### b. 适应性强\n",
    "BPE-Tokenizer 能够适应不同语言，尤其在处理低资源语言或大量新词时非常有用。它通过逐步学习子词单元，能够对语言中的新词和罕见词进行合理的分词，而不需要为每个完整单词创建词条。\n",
    "\n",
    "### c. 词汇表较小\n",
    "相比于完整词级别的词汇表，BPE 生成的词汇表更小，包含的是常见的子词单元。这种设计可以显著降低存储需求，同时覆盖更多的词汇情况。BPE 的词表通常包含常见的词根、前缀、后缀以及一些高频单词。\n",
    "\n",
    "### d. 可调的词表大小\n",
    "BPE-Tokenizer 可以根据需要控制词表大小，即可以设置最大合并次数，从而生成不同规模的词汇表。在模型训练时，可以根据硬件资源、模型需求灵活调整词表大小，达到合适的平衡。\n",
    "\n",
    "### e. 较高的计算效率\n",
    "BPE-Tokenizer 相较于其他基于复杂算法的分词器，计算效率较高，尤其在推理阶段。其逐步合并字符对的过程较为简单，生成分词结果的速度较快，便于在实际应用中使用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BPE-Tokenizer 的不足\n",
    "虽然 BPE-Tokenizer 在许多方面表现良好，但也有一些不足之处：\n",
    "\n",
    "### a.缺乏语言结构信息\n",
    "BPE 仅基于字符合并，忽略了语言的语法和词法结构，可能会生成不符合语法的分词结果。\n",
    "### b.不适合非常长的单词\n",
    "在处理非常长的单词时，BPE 可能会将其切割为多个子词单元，导致信息丢失或分词结果不直观。\n",
    "### c.无法自动识别上下文信息\n",
    "BPE 不考虑上下文，因此在不同上下文中可能会对同一单词分割成不同的子词，这在语义上可能不一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\r\n",
      "[25, 108, 82, 83, 71, 24, 25, 154, 24, 14, 10, 11, 12, 13, 25, 85, 24, 25, 140, 59, 39, 157, 87, 165, 114, 24, 25, 148, 24]\r\n",
      "vizzini : he didn ' t fall ? inconceivable !\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./python-bpe/main.py # BPE分词结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test corpus: \r\n",
      "    old older finest finer best \r\n",
      "\r\n",
      "BPE dictionary: {'__eow': 3, '__sow': 4, 'e': 5, 'r': 6, 'er': 7, 'f': 8, 'i': 9, 'n': 10, 's': 11, 't': 12, 'fi': 13, 'in': 14, 'ne': 15, 'es': 16, 'st': 17, 'o': 18, 'l': 19, 'd': 20, 'ol': 21, 'ld': 22, 'de': 23, 'b': 24, 'be': 25}\r\n",
      "example: oldest\r\n",
      "tokenize result: ['__sow', 'ol', 'de', 'st', '__eow']\r\n",
      "encode: [4, 21, 23, 17, 3]\r\n",
      "decode: oldest\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ./python-bpe/main_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67b1dc40448407e9d5d6ab3cdbdcf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0c9fa6afa049b19a899037092351cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4d1f9653f4be99a47b731c9af0fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921b92999ef842ee91df7a96c8c3e7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f826babffe9340418bcb230c4b75f19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "[31373, 995]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer)\n",
    "idx=tokenizer(\"hello world\")[\"input_ids\"]\n",
    "# idx = tokenizer(\"pneumonoultramicroscopicsilicovolcanoconiosis\")[\"input_ids\"]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
